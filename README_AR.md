
# التعرف على لغة الإشارة الأمريكية – نموذج Bi-GRU قائم على الفيديو

يهدف هذا المشروع إلى التعرف التلقائي على كلمات لغة الإشارة الأمريكية (ASL) من مقاطع الفيديو. تم استخدام مكتبة MediaPipe من Google لاستخراج نقاط المفاصل من اليدين والجسم، ثم تدريب نموذج ذكاء اصطناعي من نوع Bi-GRU باستخدام هذه البيانات.

## 📁 التقنيات المستخدمة
- بايثون
- TensorFlow و Keras
- MediaPipe
- OpenCV
- NumPy، Pandas
- Matplotlib
- Tkinter (للواجهة الرسومية)

## 📊 مجموعة البيانات
- قاعدة بيانات ASLLRP (جامعة Rutgers)
- 639 فيديو لـ 51 إشارة مختلفة
- التقسيم: 70% تدريب / 20% تحقق / 10% اختبار
📊 يمكن الاطلاع على المزيد من الإحصائيات ونتائج الأداء في [assets/Statistics.txt](./assets/Statistics.txt)


## ⚙️ خطوات المعالجة المسبقة
- إزالة نقاط الوجه ومحور Z
- استخدام فقط الكتفين، الكوعين، المعصمين و42 نقطة من اليدين
- تحويل الإحداثيات إلى نسبية (اعتمادًا على الكتف)
- ملء البيانات المفقودة باستخدام طريقة stack

## 🤖 النموذج الذكي – Bi-GRU
- نموذج Bi-GRU مكوّن من طبقتين (128 و 64 خلية)
- استخدام Masking، Dropout، BatchNormalization
- استخدام callbacks مثل EarlyStopping، ReduceLROnPlateau، ModelCheckpoint
- معالجة تفاوت عدد العينات باستخدام class_weight

## 🔍 النتائج

- RNN (مطلق):
  - الدقة على مجموعة التحقق: 38%
  - لا يوجد اختبار على فيديوهات حقيقية

- LSTM (مطلق):
  - الدقة على مجموعة التحقق: 41%
  - لا يوجد اختبار على فيديوهات حقيقية

- GRU (مطلق):
  - الدقة على مجموعة التحقق: 44%
  - فيديوهات حقيقية: 1 من 10 تم التعرّف عليها بشكل صحيح

- GRU (نسبي):
  - الدقة على مجموعة التحقق: 60%
  - فيديوهات حقيقية: 3 من 10

- **Bi-GRU (النموذج النهائي):**
  - الدقة على مجموعة التحقق: **65.5%**
  - فيديوهات حقيقية: **30 من 44 فيديو (68.2%)**


## 📦 المتطلبات

قبل تشغيل المشروع، تأكد من تثبيت الحزم التالية:

- Python 3.8 أو أحدث
- TensorFlow
- MediaPipe
- OpenCV
- Pillow (لعرض الصور)
- NumPy

لتثبيت جميع الحزم دفعة واحدة:

```bash
pip install -r requirements.txt
```

## 🧪 كيفية التشغيل (واجهة رسومية)

1. بعد تثبيت الحزم، شغّل البرنامج عبر الأمر التالي:

```bash
python sign_language_gui.py
```

2. عند تشغيل الواجهة:

- اضغط على زر **Load Video** لاختيار فيديو من جهازك.
- اضغط على **Classify Video** لبدء التحليل.
- ستظهر النتيجة في الأسفل باللون الأخضر.

> تأكد أن مسارات النموذج (`GRU_model_rel_best.keras`) وملف التصنيفات (`label_map.json`) صحيحة داخل الكود.


## 🎬 فيديوهات تجريبية

تم تضمين بعض الفيديوهات الحقيقية لإشارات لغة الإشارة الأمريكية (ASL) في مجلد [samples](./samples) لغرض الاختبار.

يمكنك تجربتها باستخدام الواجهة الرسومية:
1. اضغط على "Load Video"
2. اختر أي فيديو من العينات
3. اضغط على "Classify Video" للحصول على التنبؤ


## ⚠️ ملاحظات مهمة

- بعض الإشارات في لغة الإشارة الأمريكية لها حركات متشابهة جدًا، مما قد يؤدي إلى حدوث لبس أثناء التصنيف.
- من الأمثلة على ذلك:
  - `ANSWER` و `DIRECT`
  - `BIG` و `COUCH`
  - `ART` و `CANCEL`

في حال كان أحد الكلمتين صحيحًا والثاني خطأ، فهذا يعود إلى التشابه الحركي العالي بينهما في لغة الإشارة.



## 👨‍💻 المطوّر
عدي زيدان – جامعة باموق قلعة  
المشرفة: د. فاطمانا شنترك
